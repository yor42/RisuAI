import { BaseDirectory, readFile, readDir, writeFile, open } from "@tauri-apps/plugin-fs";
import type { OpenOptions } from "@tauri-apps/plugin-fs";
import { alertError, alertNormal, alertStore, alertWait } from "../alert";
import { LocalWriter, forageStorage, isTauri } from "../globalApi.svelte";
import { decodeRisuSave, encodeRisuSaveLegacy } from "../storage/risuSave";
import { encodeRisuSaveEnhanced, decodeRisuSaveEnhanced, createChunkingController } from "../storage/risuSaveEnhanced";
import { getDatabase, setDatabaseLite } from "../storage/database.svelte";
import { relaunch } from "@tauri-apps/plugin-process";
import { sleep } from "../util";
import { hubURL } from "../characterCards";

function getBasename(data:string){
    const baseNameRegex = /\\/g
    const splited = data.replace(baseNameRegex, '/').split('/')
    const lasts = splited[splited.length-1]
    return lasts
}

/**
 * Tauri í™˜ê²½ì—ì„œ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ëŠ” í•¨ìˆ˜
 */
async function createTauriFileStream(filePath: string, chunkSize: number = 32 * 1024 * 1024): Promise<ReadableStream<Uint8Array>> {
    const file = await open(filePath, { read: true, baseDir: BaseDirectory.AppData } as OpenOptions);
    
    return new ReadableStream<Uint8Array>({
        async start(controller) {
            try {
                let position = 0;
                const buffer = new Uint8Array(chunkSize);
                
                while (true) {
                    const bytesRead = await file.read(buffer);
                    if (bytesRead === null || bytesRead === 0) {
                        break;
                    }
                    
                    const chunk = buffer.slice(0, bytesRead);
                    controller.enqueue(chunk);
                    position += bytesRead;
                }
                
                controller.close();
            } catch (error) {
                controller.error(error);
            } finally {
                await file.close();
            }
        }
    });
}

/**
 * ì›¹ í™˜ê²½ì—ì„œ ForageStorageë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì½ëŠ” í•¨ìˆ˜
 * (ì‹¤ì œë¡œëŠ” í•œ ë²ˆì— ë¡œë“œí•˜ì§€ë§Œ ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘)
 */
async function createForageStreamingWrapper(key: string, chunkSize: number = 32 * 1024 * 1024): Promise<ReadableStream<Uint8Array>> {
    const data = await forageStorage.getItem(key) as unknown as Uint8Array;
    
    return new ReadableStream<Uint8Array>({
        start(controller) {
            try {
                // í° ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì „ì†¡
                let offset = 0;
                
                const sendNextChunk = () => {
                    if (offset >= data.length) {
                        controller.close();
                        return;
                    }
                    
                    const end = Math.min(offset + chunkSize, data.length);
                    const chunk = data.slice(offset, end);
                    controller.enqueue(chunk);
                    offset = end;
                    
                    // ë‹¤ìŒ ì²­í¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì „ì†¡ (ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”)
                    setTimeout(sendNextChunk, 0);
                };
                
                sendNextChunk();
            } catch (error) {
                controller.error(error);
            }
        }
    });
}

/**
 * íŒŒì¼ í¬ê¸°ë¥¼ ì–»ëŠ” í—¬í¼ í•¨ìˆ˜ (ê¶Œí•œ ë¬¸ì œ í•´ê²°)
 */
async function getFileSize(key: string): Promise<number> {
    if (isTauri) {
        try {
            // ì§ì ‘ íŒŒì¼ì„ ì½ì–´ì„œ í¬ê¸° í™•ì¸ (stat() ê¶Œí•œ ë¬¸ì œ íšŒí”¼)
            const data = await readFile(`assets/${key}`, { baseDir: BaseDirectory.AppData });
            return data.length;
        } catch (error) {
            console.warn(`[getFileSize] Could not get size for ${key}:`, error);
            return 0;
        }
    } else {
        try {
            const data = await forageStorage.getItem(`assets/${key}`) as unknown as Uint8Array;
            return data ? data.length : 0;
        } catch (error) {
            console.warn(`[getFileSize] Could not get size for ${key}:`, error);
            return 0;
        }
    }
}

let backupController: ReturnType<typeof createChunkingController> | null = null;

export async function SaveLocalBackup(){
    alertWait("Saving local backup...")
    const writer = new LocalWriter()
    const r = await writer.init()
    if(!r){
        alertError('Failed')
        return
    }

    // ë°±ì—… ì¤‘ë‹¨ ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„±
    backupController = createChunkingController();

    //check backup data is corrupted (skip for large data due to server limitations)
    try {
        const database = getDatabase();
        const jsonData = JSON.stringify(database);
        const dataSizeMB = (new Blob([jsonData]).size / 1024 / 1024);
        console.log(`[SaveLocalBackup] Database size: ${dataSizeMB.toFixed(2)} MB`);
        
        // Skip backup check if data is large (>50MB) to avoid 413 Content Too Large and CORS issues
        if (dataSizeMB > 50) {
            console.log(`[SaveLocalBackup] Skipping backup check - data too large (${dataSizeMB.toFixed(2)}MB)`);
            alertWait(`Saving local backup... (Skipping validation for large data: ${dataSizeMB.toFixed(2)}MB)`);
            await sleep(1000);
        } else {
            console.log(`[SaveLocalBackup] Starting backup check request to: ${hubURL}/backupcheck`);
            
            const controller = new AbortController();
            const timeoutId = setTimeout(() => controller.abort(), 15000); // 15 second timeout
            
            try {
                const startTime = Date.now();
                const corrupted = await fetch(hubURL + '/backupcheck', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: jsonData,
                    signal: controller.signal
                });
                
                clearTimeout(timeoutId);
                const requestTime = Date.now() - startTime;
                console.log(`[SaveLocalBackup] Backup check request completed in ${requestTime}ms, status: ${corrupted.status}`);
                
                if(corrupted.status === 400){
                    alertError('Failed, Backup data is corrupted')
                    return
                }
            } catch (fetchError) {
                clearTimeout(timeoutId);
                throw fetchError;
            }
        }
    } catch (error) {
        console.error('[SaveLocalBackup] Backup check request failed:', error);
        console.error('[SaveLocalBackup] Error type:', error.constructor.name);
        console.error('[SaveLocalBackup] Error message:', error.message);
        
        // Handle specific error types but continue with backup for recoverable errors
        if (error.name === 'AbortError') {
            console.warn('[SaveLocalBackup] Backup check timed out, continuing with backup...');
            alertWait('Backup validation timed out, continuing backup...');
            await sleep(1500);
        } else if (error.message && (error.message.includes('413') || error.message.includes('Content Too Large'))) {
            console.warn('[SaveLocalBackup] Data too large for server validation, continuing with backup...');
            alertWait('Data too large for validation, continuing backup...');
            await sleep(1500);
        } else if (error.message && error.message.includes('CORS')) {
            console.warn('[SaveLocalBackup] CORS error during backup check, continuing with backup...');
            alertWait('Cross-origin validation blocked, continuing backup...');
            await sleep(1500);
        } else if (error.name === 'TypeError' && error.message.includes('fetch')) {
            console.warn('[SaveLocalBackup] Network error during backup check, continuing with backup...');
            alertWait('Network error during validation, continuing backup...');
            await sleep(1500);
        } else {
            // For any other errors, log but continue with backup (server might be down)
            console.warn(`[SaveLocalBackup] Unknown error during backup check, continuing with backup: ${error.message}`);
            alertWait('Backup validation failed, but continuing backup...');
            await sleep(1500);
        }
    }

    console.log('[SaveLocalBackup] Starting streaming asset backup process...');
    const memoryBefore = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
    console.log(`[SaveLocalBackup] Memory before asset processing: ${memoryBefore.toFixed(2)}MB`);

    const STREAM_THRESHOLD_MB = 50; // 50MB ì´ìƒ íŒŒì¼ì€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    const CHUNK_SIZE = 32 * 1024 * 1024; // 32MB ì²­í¬ í¬ê¸°

    if(isTauri){
        const assets = await readDir('assets', {baseDir: BaseDirectory.AppData})
        console.log(`[SaveLocalBackup] Found ${assets.length} assets to backup`);
        let i = 0;
        let totalAssetSize = 0;
        
        for(let asset of assets){
            i += 1;
            alertWait(`Saving local Backup... (${i} / ${assets.length})`)
            const key = asset.name
            if(!key || !key.endsWith('.png')){
                continue
            }
            
            // íŒŒì¼ í¬ê¸° í™•ì¸
            const assetSize = await getFileSize(key);
            const assetSizeMB = assetSize / 1024 / 1024;
            totalAssetSize += assetSizeMB;
            
            const memoryDuring = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
            console.log(`[SaveLocalBackup] Processing asset ${i}/${assets.length}: ${key} (${assetSizeMB.toFixed(2)}MB), Memory: ${memoryDuring.toFixed(2)}MB`);
            
            // ğŸ”§ í•´ê²°ì±…: íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
            if (assetSizeMB > STREAM_THRESHOLD_MB) {
                console.log(`[SaveLocalBackup] Using streaming for large asset: ${key} (${assetSizeMB.toFixed(2)}MB)`);
                
                // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                await writer.writeBackupStream(
                    key,
                    () => createTauriFileStream(`assets/${key}`, CHUNK_SIZE),
                    assetSize,
                    (progress) => {
                        alertWait(`Saving local Backup... (${i} / ${assets.length}) - ${key}: ${progress.toFixed(1)}%`);
                    }
                );
            } else {
                // ì‘ì€ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± vs ì²˜ë¦¬ ì†ë„)
                console.log(`[SaveLocalBackup] Using traditional method for small asset: ${key} (${assetSizeMB.toFixed(2)}MB)`);
                const assetData = await readFile('assets/' + asset.name, {baseDir: BaseDirectory.AppData});
                await writer.writeBackup(key, assetData);
            }
            
            // ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (ë” ìì£¼)
            if (i % 5 === 0 && globalThis.gc) {
                globalThis.gc();
                const memoryAfterGC = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
                console.log(`[SaveLocalBackup] Memory after GC (every 5 assets): ${memoryAfterGC.toFixed(2)}MB`);
            }
        }
        console.log(`[SaveLocalBackup] Total asset data processed: ${totalAssetSize.toFixed(2)}MB`);
    }
    else{
        const keys = await forageStorage.keys()
        console.log(`[SaveLocalBackup] Found ${keys.length} keys to process`);
        let totalAssetSize = 0;

        for(let i=0;i<keys.length;i++){
            alertWait(`Saving local Backup... (${i} / ${keys.length})`)

            const key = keys[i]
            if(!key || !key.endsWith('.png')){
                continue
            }
            
            // íŒŒì¼ í¬ê¸° í™•ì¸
            const assetSize = await getFileSize(key.replace('assets/', ''));
            const assetSizeMB = assetSize / 1024 / 1024;
            totalAssetSize += assetSizeMB;
            
            const memoryDuring = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
            console.log(`[SaveLocalBackup] Processing key ${i}/${keys.length}: ${key} (${assetSizeMB.toFixed(2)}MB), Memory: ${memoryDuring.toFixed(2)}MB`);
            
            // ğŸ”§ í•´ê²°ì±…: íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
            if (assetSizeMB > STREAM_THRESHOLD_MB) {
                console.log(`[SaveLocalBackup] Using streaming wrapper for large asset: ${key} (${assetSizeMB.toFixed(2)}MB)`);
                
                // ì›¹ í™˜ê²½ì—ì„œëŠ” ìŠ¤íŠ¸ë¦¬ë° ë˜í¼ ì‚¬ìš©
                await writer.writeBackupStream(
                    key,
                    () => createForageStreamingWrapper(key, CHUNK_SIZE),
                    assetSize,
                    (progress) => {
                        alertWait(`Saving local Backup... (${i} / ${keys.length}) - ${key}: ${progress.toFixed(1)}%`);
                    }
                );
            } else {
                // ì‘ì€ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                console.log(`[SaveLocalBackup] Using traditional method for small asset: ${key} (${assetSizeMB.toFixed(2)}MB)`);
                const assetData = await forageStorage.getItem(key) as unknown as Uint8Array;
                await writer.writeBackup(key, assetData);
            }
            
            if(forageStorage.isAccount){
                await sleep(1000)
            }
            
            // ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (ë” ìì£¼)
            if (i % 5 === 0 && globalThis.gc) {
                globalThis.gc();
                const memoryAfterGC = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
                console.log(`[SaveLocalBackup] Memory after GC (every 5 assets): ${memoryAfterGC.toFixed(2)}MB`);
            }
        }
        console.log(`[SaveLocalBackup] Total asset data processed: ${totalAssetSize.toFixed(2)}MB`);
    }

    const memoryAfter = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
    console.log(`[SaveLocalBackup] Memory after streaming asset processing: ${memoryAfter.toFixed(2)}MB (diff: +${(memoryAfter - memoryBefore).toFixed(2)}MB)`);

    // ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì¸ì½”ë”©
    const database = getDatabase();
    console.log('[SaveLocalBackup] Starting database encoding with chunking...');
    
    let dbData: Uint8Array;
    try {
        // Enhanced encoding with progress callback and abort support
        dbData = await encodeRisuSaveEnhanced(database, (progress, stage, current, total) => {
            if (current && total) {
                alertWait(`Saving local Backup... (Database: ${stage} ${current}/${total}) [Click to cancel]`);
            } else {
                alertWait(`Saving local Backup... (Database: ${stage} ${progress.toFixed(1)}%) [Click to cancel]`);
            }
        }, backupController);
        console.log(`[SaveLocalBackup] Enhanced encoding completed: ${(dbData.length / 1024 / 1024).toFixed(2)}MB`);
    } catch (error) {
        if (error.message?.includes('aborted')) {
            console.log('[SaveLocalBackup] Backup was cancelled by user');
            alertError('Backup cancelled by user');
            backupController = null;
            await writer.close();
            return;
        }
        console.warn('[SaveLocalBackup] Enhanced encoding failed, using legacy method:', error);
        alertWait(`Saving local Backup... (Saving database - fallback mode)`);
        dbData = encodeRisuSaveLegacy(database, 'compression');
        console.log(`[SaveLocalBackup] Legacy encoding completed: ${(dbData.length / 1024 / 1024).toFixed(2)}MB`);
    }

    alertWait(`Saving local Backup... (Writing database file)`);
    await writer.writeBackup('database.risudat', dbData);

    alertNormal('Success')
    backupController = null;
    await writer.close()
}

/**
 * ë°±ì—… ì‘ì—… ì¤‘ë‹¨
 */
export function cancelBackup() {
    if (backupController) {
        console.log('[SaveLocalBackup] Cancelling backup operation...');
        backupController.abort();
    }
}

let restoreController: ReturnType<typeof createChunkingController> | null = null;

export async function LoadLocalBackup(){
    try {
        const input = document.createElement('input');
        input.type = 'file';
        input.accept = '.bin';
        input.onchange = async () => {
            if (!input.files || input.files.length === 0) {
                input.remove();
                return;
            }
            const file = input.files[0];
            input.remove();

            // ë³µì› ì¤‘ë‹¨ ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„±
            restoreController = createChunkingController();

            const reader = file.stream().getReader();
            const CHUNK_SIZE = 1024 * 1024; // 1MB chunk size
            let bytesRead = 0;
            let remainingBuffer = new Uint8Array();

            while (true) {
                const { done, value } = await reader.read();
                if (done) {
                    break;
                }

                bytesRead += value.length;
                const progress = ((bytesRead / file.size) * 100).toFixed(2);
                alertWait(`Loading local Backup... (${progress}%)`);

                const newBuffer = new Uint8Array(remainingBuffer.length + value.length);
                newBuffer.set(remainingBuffer);
                newBuffer.set(value, remainingBuffer.length);
                remainingBuffer = newBuffer;

                let offset = 0;
                while (offset + 4 <= remainingBuffer.length) {
                    const nameLength = new Uint32Array(remainingBuffer.slice(offset, offset + 4).buffer)[0];

                    if (offset + 4 + nameLength > remainingBuffer.length) {
                        break;
                    }
                    const nameBuffer = remainingBuffer.slice(offset + 4, offset + 4 + nameLength);
                    const name = new TextDecoder().decode(nameBuffer);

                    if (offset + 4 + nameLength + 4 > remainingBuffer.length) {
                        break;
                    }
                    const dataLength = new Uint32Array(remainingBuffer.slice(offset + 4 + nameLength, offset + 4 + nameLength + 4).buffer)[0];

                    if (offset + 4 + nameLength + 4 + dataLength > remainingBuffer.length) {
                        break;
                    }
                    const data = remainingBuffer.slice(offset + 4 + nameLength + 4, offset + 4 + nameLength + 4 + dataLength);

                    if (name === 'database.risudat') {
                        const db = new Uint8Array(data);
                        console.log(`[LoadLocalBackup] Decoding database: ${(db.length / 1024 / 1024).toFixed(2)}MB`);
                        
                        let dbData: any;
                        try {
                            // Enhanced decoding with progress callback and abort support
                            dbData = await decodeRisuSaveEnhanced(db, (progress, stage) => {
                                alertWait(`Loading local Backup... (Database: ${stage} ${progress.toFixed(1)}%) [Click to cancel]`);
                            }, restoreController);
                            console.log('[LoadLocalBackup] Enhanced decoding completed');
                        } catch (error) {
                            if (error.message?.includes('aborted')) {
                                console.log('[LoadLocalBackup] Restore was cancelled by user');
                                alertError('Restore cancelled by user');
                                restoreController = null;
                                return;
                            }
                            console.warn('[LoadLocalBackup] Enhanced decoding failed, using legacy method:', error);
                            alertWait(`Loading local Backup... (Database decoding - fallback mode)`);
                            dbData = await decodeRisuSave(db);
                            console.log('[LoadLocalBackup] Legacy decoding completed');
                        }
                        
                        setDatabaseLite(dbData);
                        if (isTauri) {
                            await writeFile('database/database.bin', db, { baseDir: BaseDirectory.AppData });
                            await relaunch();
                            alertStore.set({
                                type: "wait",
                                msg: "Success, Refreshing your app."
                            });
                        } else {
                            await forageStorage.setItem('database/database.bin', db);
                            location.search = '';
                            alertStore.set({
                                type: "wait",
                                msg: "Success, Refreshing your app."
                            });
                        }
                    } else {
                        // ğŸ”§ í•´ê²°ì±…: Asset íŒŒì¼ë„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ê²½ìš°)
                        const assetSizeMB = data.length / 1024 / 1024;
                        const RESTORE_STREAM_THRESHOLD_MB = 50;
                        
                        console.log(`[LoadLocalBackup] Restoring asset ${name}: ${assetSizeMB.toFixed(2)}MB`);
                        const memoryBefore = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
                        
                        if (assetSizeMB > RESTORE_STREAM_THRESHOLD_MB) {
                            console.log(`[LoadLocalBackup] Using streaming for large asset restore: ${name} (${assetSizeMB.toFixed(2)}MB)`);
                            
                            // í° íŒŒì¼ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥
                            if (isTauri) {
                                const file = await open(`assets/${name}`, {
                                    write: true,
                                    create: true,
                                    truncate: true,
                                    baseDir: BaseDirectory.AppData
                                } as OpenOptions);
                                
                                try {
                                    const chunkSize = 32 * 1024 * 1024; // 32MB chunks
                                    let chunkOffset = 0;
                                    let chunkCount = 0;
                                    
                                    while (chunkOffset < data.length) {
                                        const end = Math.min(chunkOffset + chunkSize, data.length);
                                        const chunk = data.slice(chunkOffset, end);
                                        await file.write(chunk);
                                        chunkOffset = end;
                                        chunkCount++;
                                        
                                        // Progress update for large files
                                        if (chunkCount % 10 === 0) {
                                            const progress = (chunkOffset / data.length) * 100;
                                            alertWait(`Loading local Backup... (Restoring ${name}: ${progress.toFixed(1)}%)`);
                                        }
                                        
                                        // Allow other operations to run
                                        await sleep(1);
                                    }
                                    console.log(`[LoadLocalBackup] Streamed ${chunkCount} chunks for ${name}`);
                                } finally {
                                    await file.close();
                                }
                            } else {
                                // ì›¹ í™˜ê²½ì—ì„œëŠ” ì—¬ì „íˆ ì „ì²´ ì €ì¥ (forageStorage ì œí•œ)
                                // í•˜ì§€ë§Œ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ UI ë¸”ë¡œí‚¹ ë°©ì§€
                                const chunkSize = 32 * 1024 * 1024;
                                const chunks: Uint8Array[] = [];
                                let chunkOffset = 0;
                                
                                while (chunkOffset < data.length) {
                                    const end = Math.min(chunkOffset + chunkSize, data.length);
                                    chunks.push(data.slice(chunkOffset, end));
                                    chunkOffset = end;
                                    await sleep(1); // UI ë¸”ë¡œí‚¹ ë°©ì§€
                                }
                                
                                // ìµœì¢…ì ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì €ì¥
                                await forageStorage.setItem('assets/' + name, data);
                                console.log(`[LoadLocalBackup] Processed ${chunks.length} chunks for web storage: ${name}`);
                            }
                        } else {
                            // ì‘ì€ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                            console.log(`[LoadLocalBackup] Using traditional method for small asset: ${name} (${assetSizeMB.toFixed(2)}MB)`);
                            if (isTauri) {
                                await writeFile(`assets/` + name, data, { baseDir: BaseDirectory.AppData });
                            } else {
                                await forageStorage.setItem('assets/' + name, data);
                            }
                        }
                        
                        const memoryAfter = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
                        console.log(`[LoadLocalBackup] Asset ${name} restored, Memory: ${memoryAfter.toFixed(2)}MB (diff: +${(memoryAfter - memoryBefore).toFixed(2)}MB)`);
                    }
                    await sleep(10);
                    if (forageStorage.isAccount) {
                        await sleep(1000);
                    }

                    offset += 4 + nameLength + 4 + dataLength;
                    
                    // ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (í° íŒŒì¼ ì²˜ë¦¬ í›„)
                    if (dataLength > 50 * 1024 * 1024 && globalThis.gc) { // 50MB ì´ìƒ íŒŒì¼ í›„
                        globalThis.gc();
                        const memoryAfterGC = (performance as any).memory ? (performance as any).memory.usedJSHeapSize / 1024 / 1024 : 0;
                        console.log(`[LoadLocalBackup] Memory after GC (large file processed): ${memoryAfterGC.toFixed(2)}MB`);
                    }
                }
                remainingBuffer = remainingBuffer.slice(offset);
            }

            alertNormal('Success');
            restoreController = null;
        };

        input.click();
    } catch (error) {
        console.error(error);
        alertError('Failed, Is file corrupted?')
        restoreController = null;
    }
}

/**
 * ë³µì› ì‘ì—… ì¤‘ë‹¨
 */
export function cancelRestore() {
    if (restoreController) {
        console.log('[LoadLocalBackup] Cancelling restore operation...');
        restoreController.abort();
    }
}